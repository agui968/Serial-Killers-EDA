{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Es determinante el entorno  para los asesinos en serie?\n",
    "##### Memoria del proyecto\n",
    "\n",
    "Fuentes de datos y datasets utilizados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 9, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Agui\\Documents\\Profesional\\01_Formacion\\01_TB_bcmp_DS\\Core curriculum_Student\\Entregas\\EDA\\src\\memo.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Agui/Documents/Profesional/01_Formacion/01_TB_bcmp_DS/Core%20curriculum_Student/Entregas/EDA/src/memo.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Agui/Documents/Profesional/01_Formacion/01_TB_bcmp_DS/Core%20curriculum_Student/Entregas/EDA/src/memo.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://www.kaggle.com/datasets/vesuvius13/serial-killers-dataset/data?select=Highest_victim_count.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m#highest victim count\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Agui/Documents/Profesional/01_Formacion/01_TB_bcmp_DS/Core%20curriculum_Student/Entregas/EDA/src/memo.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m fuente_1 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.kaggle.com/datasets/vesuvius13/serial-killers-dataset/data?select=Highest_victim_count.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Agui/Documents/Profesional/01_Formacion/01_TB_bcmp_DS/Core%20curriculum_Student/Entregas/EDA/src/memo.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df_2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(\u001b[39m\"\u001b[39m\u001b[39mhttps://query.data.world/s/xtork6gpqastc7w63cca7pzdkgi5kh?dws=00000\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m#population density\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 9, saw 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_1 = pd.read_csv(\"https://www.kaggle.com/datasets/vesuvius13/serial-killers-dataset/data?select=Highest_victim_count.csv\") #highest victim count\n",
    "fuente_1 = \"https://www.kaggle.com/datasets/vesuvius13/serial-killers-dataset/data?select=Highest_victim_count.csv\"\n",
    "\n",
    "df_2 = pd.read_json(\"https://query.data.world/s/xtork6gpqastc7w63cca7pzdkgi5kh?dws=00000\") #population density\n",
    "fuente_2 =\"https://data.world/samayo/country-names/workspace/file?filename=country-population-density.json\"\n",
    "\n",
    "df_3 = pd.read_csv('serialkillers_full\\\\poverty_rate.csv') #poverty rate\n",
    "fuente_3 = \"https://data.oecd.org/inequality/poverty-rate.htm\"\n",
    "\n",
    "df_4 = pd.read_csv(\"https://www.kaggle.com/datasets/vesuvius13/serial-killers-dataset/data?select=15_to_30_victim_count.csv\")\n",
    "fuente_4 = \"https://www.kaggle.com/datasets/vesuvius13/serial-killers-dataset/data?select=15_to_30_victim_count.csv\"\n",
    "\n",
    "df_5 = pd.read_csv(\"https://gist.githubusercontent.com/radcliff/f09c0f88344a7fcef373/raw/2753c482ad091c54b1822288ad2e4811c021d8ec/wikipedia-iso-country-codes.csv\")\n",
    "fuente_5 = \"https://gist.githubusercontent.com/radcliff/f09c0f88344a7fcef373/raw/2753c482ad091c54b1822288ad2e4811c021d8ec/wikipedia-iso-country-codes.csv\"\n",
    "\n",
    "df_6 = pd.read_csv('https://www.kaggle.com/datasets/vesuvius13/serial-killers-dataset/data?select=5_to_14_victim_count.csv')\n",
    "\n",
    "df_7 = pd.read_csv('https://www.kaggle.com/datasets/vesuvius13/serial-killers-dataset/data?select=Lessthan_5_victim_count.csv')\n",
    "\n",
    "#CAMBIAR RUTAS a archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# añadir heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza de los cuatro datasets de los sujetos\n",
    "(Este proceso desarrollado se encuentra en el notebook \"1datacleaning.ipynb\").\n",
    "\n",
    "+ El primer paso, y el más laborioso, fue el de limpiar los datos.\n",
    "+ La columna de países de procedencia de los sujetos incluía varios valores en la misma celda (además de datos como \"Brasil (supuestamente)\"), por lo que he tenido que limpiarla separando los valores en una lista (```str.split()```) y dividiéndolos en filas con ``` pd.explode()```. De esta forma, si un sujeto perpetró crímenes en tres países, aparecerá tres veces, ya que lo que nos importa realmente son los países en los que tienen lugar dichos crímenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hivict['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating countries in the same cell as a list\n",
    "hivict['country']=hivict['country'].str.split('\\r\\n')\n",
    "\n",
    "#dividing list in several rows and removing the \" (alleged)\" bit\n",
    "hivictexp=hivict.explode('country')\n",
    "hivictexp['country']=hivictexp['country'].str.replace(\" (alleged)\",\"\")\n",
    "hivictexp['country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proceso se repite para las cuatro tablas de datos de asesinos, que posteriormente se concatenarán.\n",
    "\n",
    "Conforme iba obteniendo los datos de países limpios, los iba guardando en archivos csv definitivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza de los datasets de datos demográficos y la clave de país-código ISO\n",
    "\n",
    "(Este proceso desarrollado se encuentra en el notebook \"1datacleaning.ipynb\").\n",
    "\n",
    "+ Lo primero que tuve que hacer fue homogeinizar los nombres de las columnas y eliminar las que eran irrelevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#añadir head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homogeneizar nombres de columnas\n",
    "for col in povrates:\n",
    "    povrates.rename(columns={col:col.lower()}, inplace=True)\n",
    "\n",
    "povrates.rename(columns={'location':'code', 'time':'year'}, inplace=True)\n",
    "\n",
    "povrates=povrates[['code', 'year','value']]\n",
    "povrates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Principalmente quería basarme en dos datos demográficos: **la densidad de población y el índice de pobreza**.Sin embargo, el dataset de *poverty rates* incluía el código ISO de los países en lugar de su nombre, lo que me obligó a buscar otro csv que me permitiera hacer la correspondencia entre ambos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countrycode=countrycode[['English short name lower case', 'Alpha-3 code']]\n",
    "countrycode.rename(columns={'English short name lower case':'country name', 'Alpha-3 code': 'code'},inplace=True)\n",
    "countrycode.sort_values(['code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Después tuve que limpiar el dataset de índice de pobreza, que tenía varios valores según el año, pero muchos NaN también. **Decidí agruparlos por país y hacer una media del índice de pobreza, para dejar solo una fila por país, que después sería una nueva columna del dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpovratecountry=povrates.groupby('code')['value'].mean()\n",
    "avgpovratecountry=pd.DataFrame(avgpovratecountry).reset_index()\n",
    "avgpovratecountry.rename(columns={'value':'mean value'},inplace=True)\n",
    "avgpovratecountry.drop(columns=['level_0','index'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Una vez hecho esto, se podía eliminar la columna de año y los valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "povrates.dropna(subset=['value'],inplace=True)\n",
    "povrates.dropna(subset=['year'],inplace=True)\n",
    "simplepovrates=povrates.drop(columns='year').drop_duplicates('code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ A partir de ahí, tuve que empezar a utilizar ```pd.merge()``` para juntar los datos demográficos y la clave de país-código ISO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanpovrate=simplepovrates.merge(avgpovratecountry, on='code').drop(columns='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Se añade el nombre de los países a *pov rates* haciendo un merge con la anterior tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countrypovrate=pd.merge(cleanpovrate, countrycode, how='outer')\n",
    "countrypovrate.dropna(subset=['mean value'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homogeinizar los nombres de los países en los distintos datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ **Ahí me di cuenta de que algunos países del dataset de asesinos no estaban escritos igual que los de este dataset** (por ejemplo, 'Korea, Republic of', frente a 'South Korea').\n",
    "+ Esto me obligó a crear un diccionario para corregir y homogeninizar los nombres de los países:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongpovratecountries={'Korea, Republic of':'South Korea'}\n",
    "\n",
    "for wrong,right in wrongpovratecountries.items():\n",
    "    countrypovrate['country name']=countrypovrate['country name'].str.replace(wrong, right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ El mismo procedimiento se repitió para el dataest de la densidad de población:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongpopdenscountries={'Iran, Islamic Rep.':'Iran','Russian Federation':'Russia','Korea, Rep.':'South Korea',\n",
    "                       'Venezuela, RB':'Venezuela','Yemen, Rep.':'Yemen'}\n",
    "\n",
    "for wrong,right in wrongpovratecountries.items():\n",
    "    popdens['country']=popdens['country'].str.replace(wrong, right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Este proceso desarrollado se encuentra en el notebook \"2datacleaning_matching.ipynb\").\n",
    "\n",
    "+ Se concatenan los cuatro datasets de datos de asesinos con ```pd.concat()``` y se ve los países cuyo nombre no encaja con los otros datasets (para hacer otro diccionario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys are wrong, values are the correct ones (in killer df)\n",
    "wrongkillercountries={'Allied-occupied Germany':'Germany','Austria-Hungary':'Hungary',\n",
    "                      'Czechoslovakia':'Czech Republic','East Germany':'Germany','German Empire':'Germany',\n",
    "                      'Kingdom of Romania':'Romania','Ottoman Empire':'Iran','Portuguese Angola':'Angola',\n",
    "                      'West Germany':'Germany', 'Soviet Union':'Ukraine', 'Yugoslavia':'Serbia'}\n",
    "\n",
    "for wrong,right in wrongkillercountries.items():\n",
    "    totalvict['country']=totalvict['country'].str.replace(wrong, right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Este proceso desarrollado se encuentra en el notebook \"3mergingdf.ipynb\").\n",
    "\n",
    "+ Por último, en un último Jupyter notebook de limpieza se fusionan todos los datasets para poder trabajar con un dataframe definitivo y se eliminan las columnas innecesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged killers df with poverty rate df\n",
    "kill_povrate=killers.merge(povrate, how='left')\n",
    "\n",
    "kill_povrate['mean value'].isnull().value_counts() #quite a few nans for poverty rate\n",
    "\n",
    "#remove \"code\" column\n",
    "kill_povrate.drop(columns='code', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged killers_povrate df with pop density df\n",
    "dfkillers=kill_povrate.merge(popdens, how='left')\n",
    "dfkillers.rename(columns={'mean value':'pov rate'},inplace=True)\n",
    "\n",
    "dfkillers.to_csv(\"..\\\\data\\\\cleaned\\\\eda_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ El *eda_df.csv* ya está listo para el análisis. Veamos el resultado final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfkillers[(dfkillers['density'].notnull()) & (dfkillers['pov rate'].notnull())])\n",
    "#201 killers with both data\n",
    "#pov rate: 205 values, 150 nan\n",
    "#density: 313 values, 42 nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Hay 150 países sin datos de índice de pobreza y 42 países sin datos de densidad de población. **Es decir, disponemos de todos los datos de 201 sujetos**.\n",
    "+ Se exporta otro csv sin NaN también:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notnullkillers=dfkillers[(dfkillers['density'].notnull()) & (dfkillers['pov rate'].notnull())]\n",
    "notnullkillers.to_csv(\"..\\\\data\\\\cleaned\\\\notnull_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de los datos y comprobación de hipótesis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
